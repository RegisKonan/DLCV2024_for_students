{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical flow estimation\n",
    "\n",
    "In this lab we will be working on the optical flow estimation with \n",
    "<a href=\"https://en.wikipedia.org/wiki/Lucas%E2%80%93Kanade_method\"> Lucas Kanade algorithm</a>. It is a very well known algorithm based on derivative estimation.\n",
    "   \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first analyse our own implementation of a dense estimator, we will then compare it with the OpenCV implementation which is a sparse one (computed on corners only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from visualize_flow import flow_to_color\n",
    "import time\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Inside the folder 'Data' you will find different image sequences, they capture different type of motion. Have a look at them first, and choose with what type of data you would like to work.\n",
    "\n",
    "The input of optical flow is a pair of subsequent image frames, in fact, given an image sequence, you could try out different pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AND VISUALIZE AN IMAGE PAIR\n",
    "\n",
    "#img1= cv2.imread('Data/EmptyScene01.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "#img2= cv2.imread('Data/EmptyScene02.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "img1= cv2.imread('Data/waving/JPEGImages/daria_wave2004.png',cv2.IMREAD_GRAYSCALE)\n",
    "img2= cv2.imread('Data/waving/JPEGImages/daria_wave2015.png',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A first analysis: change detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=cv2.absdiff(img1,img2)\n",
    "\n",
    "plt.imshow(D,cmap='gray')\n",
    "plt.title('Difference')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lucas Kanade: our dense implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lucas_Kanade(im1,im2,window_size):\n",
    "    kernel_x = np.array([[-1., 1.], [-1., 1.]])*.25 \n",
    "    kernel_y = np.array([[-1., -1.], [1., 1.]])*.25\n",
    "    kernel_t = np.array([[1., 1.], [1., 1.]])*.25\n",
    "    w = int(window_size/2) # window_size is odd, all the pixels with offset in between [-w, w] are inside the window\n",
    "    I1g = im1 / 255. # normalize pixels\n",
    "    I2g = im2 / 255. # normalize pixels\n",
    "    # Implement Lucas Kanade\n",
    "    # for each point, calculate I_x, I_y, I_t\n",
    "    mode = 'same'\n",
    "    fx = signal.convolve2d(I1g, kernel_x, boundary='symm', mode=mode)# + signal.convolve2d(I2g, kernel_x, boundary='symm', mode=mode)\n",
    "    fy = signal.convolve2d(I1g, kernel_y, boundary='symm', mode=mode)# + signal.convolve2d(I2g, kernel_y, boundary='symm', mode=mode) \n",
    "    ft = signal.convolve2d(I1g, kernel_t, boundary='symm', mode=mode) +signal.convolve2d(I2g, -kernel_t, boundary='symm', mode=mode)\n",
    "   \n",
    "    u = np.zeros(I1g.shape)\n",
    "    v = np.zeros(I1g.shape)\n",
    "   \n",
    "    # within window window_size * window_size\n",
    "    \n",
    "    for i in range(w, I1g.shape[0]-w):\n",
    "        for j in range(w, I1g.shape[1]-w):\n",
    "            #{To Be filled by the student with a hint on pinv\n",
    "            Ix = fx[i-w:i+w+1, j-w:j+w+1].transpose().flatten()\n",
    "            Iy = fy[i-w:i+w+1, j-w:j+w+1].transpose().flatten()\n",
    "            It = np.array(ft[i-w:i+w+1, j-w:j+w+1].transpose().flatten())\n",
    "            \n",
    "            A = np.transpose(np.array([Ix ,Iy]))\n",
    "            \n",
    "            AT =A.transpose()\n",
    "            \n",
    "            ATA = np.matmul(A,AT)\n",
    "            It = -It\n",
    "            \n",
    "            Inv = np.linalg.pinv(ATA)\n",
    "            U = np.matmul(np.matmul(Inv,A).transpose(),It)\n",
    "            u[i,j]=U[0]\n",
    "            v[i,j]=U[1]\n",
    "             \n",
    "\n",
    "    return (u,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the previously defined function on the image pair and <b> reason on the impact of different window sizes </b> (notice the larger the window the slowlier the method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "window=3\n",
    "[u,v] = Lucas_Kanade(img1,img2,window)\n",
    "print(\"Elapsed time is %d seconds\"%(time.time()-start) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results of the flow field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "print(u.shape)\n",
    "print(v.shape)\n",
    "xaxis = list(np.arange(img1.shape[0]))\n",
    "yaxis = list(np.arange(img1.shape[1]))\n",
    "plt.quiver(u,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the flow field using a color-coding algorithm ( See https://github.com/tomrunia/OpticalFlow_Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flow = np.stack([u,v],axis=2)\n",
    "plt.imshow(flow_to_color(flow,convert_to_bgr=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILL IN HERE\n",
    "\n",
    "# fix img1 and load two or three different versions of img2 (the following frame)\n",
    "# and more distant ones\n",
    "# load the optical flow you obtain for at least 3 choices of window\n",
    "\n",
    "\n",
    "img1= cv2.imread('Data/waving/JPEGImages/daria_wave2004.png',cv2.IMREAD_GRAYSCALE)\n",
    "img2a= cv2.imread('Data/waving/JPEGImages/daria_wave2005.png',cv2.IMREAD_GRAYSCALE)\n",
    "img2b= cv2.imread('Data/waving/JPEGImages/daria_wave2015.png',cv2.IMREAD_GRAYSCALE)\n",
    "img2c= cv2.imread('Data/waving/JPEGImages/daria_wave2025.png',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "window=5\n",
    "[u1,v1] = Lucas_Kanade(img1,img2a,window)\n",
    "flow1 = np.stack([u1,v1],axis=2)\n",
    "[u2,v2] = Lucas_Kanade(img1,img2b,window)\n",
    "flow2 = np.stack([u2,v2],axis=2)\n",
    "[u3,v3] = Lucas_Kanade(img1,img2c,window)\n",
    "flow3 = np.stack([u3,v3],axis=2)\n",
    " \n",
    "window=10\n",
    "[u11,v11] = Lucas_Kanade(img1,img2a,window)\n",
    "flow11 = np.stack([u11,v11],axis=2)\n",
    "[u21,v21] = Lucas_Kanade(img1,img2b,window)\n",
    "flow21 = np.stack([u21,v21],axis=2)\n",
    "[u31,v31] = Lucas_Kanade(img1,img2c,window)\n",
    "flow31 = np.stack([u31,v31],axis=2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(231)\n",
    "plt.imshow(flow_to_color(flow1,convert_to_bgr=False))\n",
    "plt.subplot(232)\n",
    "plt.imshow(flow_to_color(flow2,convert_to_bgr=False))\n",
    "plt.subplot(233)\n",
    "plt.imshow(flow_to_color(flow3,convert_to_bgr=False))\n",
    "plt.subplot(234)\n",
    "plt.imshow(flow_to_color(flow11,convert_to_bgr=False))\n",
    "plt.subplot(235)\n",
    "plt.imshow(flow_to_color(flow21,convert_to_bgr=False))\n",
    "plt.subplot(236)\n",
    "plt.imshow(flow_to_color(flow31,convert_to_bgr=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lucas Kanade: sparse OpenCV implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm evaluates optical flow on sparse points (corners) in order to avoid the ill-posed inversion of A'A.\n",
    "Additionally, Optical flow is calculated and combined on different scales (hierarchical implementation) to handle large-displacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters of the corner detection procedure\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.1,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Call the function that detects the key-points (Shi-Tomasi corners) from the first frame \n",
    "2- Call LK Flow algorithm which returns the positions of these key points in the second frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = cv2.goodFeaturesToTrack(img1, mask = None, **feature_params)\n",
    "p1, st, err = cv2.calcOpticalFlowPyrLK(img1, img2, p0, None, **lk_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw vectors to connect points from the first frame and the second frame to visualize the motion vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good_new = p1[st==1]\n",
    "#good_old = p0[st==1]\n",
    "mask = np.zeros_like(img1)\n",
    "#print(good_old.shape)\n",
    "for i,(new,old) in enumerate(zip(p1,p0)):\n",
    "    a,b = np.int32(new.ravel())\n",
    "    \n",
    "    c,d = np.int32(old.ravel())\n",
    "    mask = cv2.line(mask, (a,b),(c,d), [255,255,0], 2)\n",
    "img2 = cv2.add(img2,mask)\n",
    "plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished up already? Try and extend the sparse version visualization to incorporate the estimates on multiple adjacent frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense vs sparse\n",
    "How do you feel about these two options? What do you think is best for different applications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take-home messages:\n",
    "\n",
    "### Sparse vs Dense\n",
    "- with a dense estimate we obtain a full estimate of the motion, but it can be noisy (in particular in uniform areas). \n",
    "- conversely, a sparse approach focuses on reliable/stable points (not affected by the aperture problem) but it provides a lot less information\n",
    "\n",
    "### Size of the window\n",
    "- These implementations of LK are based on local constancy of motion assumption, which may be wrong in general. From this point of view, small windows are better.\n",
    "- Also, larger windows are computationally more expensive\n",
    "- At the same time with larger windows we obtain a smoother estimate (did you notice this?)\n",
    "\n",
    "## A final question\n",
    "\n",
    "\n",
    "- Could we consider the use of this algorithm (dense version) to estimate a disparity map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
